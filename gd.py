
# Let's create a super simple linear-ish data set, and fit a linear regression
# model to it 4 different ways:
#
# 1. By using the "Normal Equation" (with Moore-Penrose Pseudoinverse, as in
#   lrByHand.py)
# 2. By using sklearn's LinearRegression (duh).
# 3. By converging on the best coefficients/weights using Gradient Descent.

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.random.seed(123)

N = 1000    # number of training points. (Eschewing GÃ©ron's syntax)

np.set_printoptions(precision=4)
np.set_printoptions(floatmode='fixed')

true_x0_slope = 17
true_x1_slope = 2.43

print(f"True weights:    {np.array([true_x0_slope, true_x1_slope])}")

X = 2*np.random.rand(N,2)
y = true_x0_slope * X[:,0:1] + true_x1_slope * X[:,1:2] + \
    np.random.normal(0,1,size=(N,1))


# 1. Use Normal Equation.
moore_penrose_pseudo_inv = np.linalg.inv(X.T @ X) @ X.T
weights_best = moore_penrose_pseudo_inv @ y
print(f"Normal equation: {weights_best.ravel()}")



# 2. Use sklearn's LinearRegression.
from sklearn.linear_model import LinearRegression
lr = LinearRegression(fit_intercept=False)
lr.fit(X,y)
print(f"sklearn LR:      {lr.coef_[0]}")



# 3. Use Gradient Descent.
# Start off with random (wacky) weights.
wacky_initial_weight_guesses = np.array([-94, 18.2])
weights = wacky_initial_weight_guesses
eta = .05       # Our learning rate.
n_iter = 100    # We'll make this many adjustments to the weights.

# (Keep track of the weights as they're adjusted over time, for plotting.)
weights_hist = np.empty((n_iter,len(weights)))

for i in range(1,n_iter):

    # Why is the gradient the following equation? Because if you symbolically
    #   differentiate the MSE (mean-squared error) surface generated by some
    #   value of the weights, these are your partial derivatives.
    gradient = 2/N * X.T @ (X @ weights - y.ravel())

    # Nudge our weights in the right direction, using eta as a learning rate.
    weights -= eta * gradient

    # (Record the new intercept and slope, for later plotting.)
    weights_hist[i,:] = weights

print(f"Gradient Desc:   {weights_hist[-1,:]}")


fig, axs = plt.subplots(2,1)
axs[0].plot(weights_hist[:,0])
axs[0].axhline(y=true_x0_slope, color="red")
axs[0].set_ylim(min(true_x0_slope, weights_hist[:,0].min()) - 5,
    max(true_x0_slope,weights_hist[:,0].max()) + 5)
axs[0].set_ylabel("Estimated intercept")
axs[1].plot(weights_hist[:,1])
axs[1].axhline(y=true_x1_slope, color="red")
axs[1].set_ylim(weights_hist[:,1].min() - 5, weights_hist[:,1].max() + 5)
axs[1].set_ylim(min(true_x1_slope, weights_hist[:,1].min()) - 5,
    max(true_x1_slope,weights_hist[:,1].max()) + 5)
axs[1].set_xlabel("Number of iterations")
axs[1].set_ylabel("Estimated x0 slope")
fig.suptitle("Gradient Descent -- weight convergence")
plt.show()

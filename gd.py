
# Let's create a super simple linear-ish data set, and fit a linear regression
# model to it 4 different ways:
#
# 1. By using the "Normal Equation" (with Moore-Penrose Pseudoinverse, as in
#   lrByHand.py)
# 2. By using sklearn's LinearRegression (duh).
# 3. By converging on the best coefficients/weights using Gradient Descent.
# 4. By using Stochastic Gradient Descent (with adjustable batch size b.)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

np.random.seed(123)

N = 1000    # number of training points. (Eschewing GÃ©ron's syntax)
d = 3       # number of features ("dimension")

np.set_printoptions(precision=4)
np.set_printoptions(floatmode='fixed')

true_weights = np.random.uniform(-50,50,d).reshape(d,1)

print(f"True weights:    {true_weights}")

X = np.random.rand(N,len(true_weights))
ss = StandardScaler()
X = ss.fit_transform(X)
y = X @ true_weights + np.random.normal(0,1,size=(N,1))


# 1. Use Normal Equation.
moore_penrose_pseudo_inv = np.linalg.inv(X.T @ X) @ X.T
weights_best = moore_penrose_pseudo_inv @ y
print(f"Normal equation: {weights_best.ravel()}")



# 2. Use sklearn's LinearRegression.
from sklearn.linear_model import LinearRegression
lr = LinearRegression(fit_intercept=False)
lr.fit(X,y)
print(f"sklearn LR:      {lr.coef_[0]}")



# 3. Use (plain-ol') Gradient Descent.
# Start off with random (wacky) weights.
wacky_initial_weight_guesses = np.random.uniform(-50,50,d).reshape(d,1)
weights = wacky_initial_weight_guesses
eta = .05       # Our learning rate.
n_iter = 100    # We'll make this many adjustments to the weights.

# (Keep track of the weights as they're adjusted over time, for plotting.)
weights_hist = np.empty((n_iter,len(weights)))

for i in range(1,n_iter):

    # Why is the gradient the following equation? Because if you symbolically
    #   differentiate the MSE (mean-squared error) surface generated by some
    #   value of the weights, these are your partial derivatives.
    gradient = 2/N * X.T @ (X @ weights - y)

    # Nudge our weights in the right direction, using eta as a learning rate.
    weights -= eta * gradient

    # (Record the new weights, for later plotting.)
    weights_hist[i,:] = weights.ravel()

print(f"Gradient Desc:   {weights_hist[-1,:]}")



# 4. Use Stochastic Gradient Descent.
# Start off with random (wacky) weights.
wacky_initial_weight_guesses = np.random.uniform(-50,50,d).reshape(d,1)
weights = wacky_initial_weight_guesses
b = 1            # Size of mini-batch.
eta = .05        # Our learning rate.
n_iter = 10000   # We'll make this many adjustments to the weights.

# (Keep track of the weights as they're adjusted over time, for plotting.)
weights_hist = np.empty((n_iter,len(weights)))

for i in range(1,n_iter):

    indices = np.random.choice(X.shape[0],b,replace=False)
    myX = X[indices, :]
    myy = y[indices]
    # Why is the gradient the following equation? Because if you symbolically
    #   differentiate the MSE (mean-squared error) surface generated by some
    #   value of the weights, these are your partial derivatives.
    gradient = 2/N * myX.T @ (myX @ weights - myy)

    # Nudge our weights in the right direction, using eta as a learning rate.
    weights -= eta * gradient

    # (Record the new weights, for later plotting.)
    weights_hist[i,:] = weights.T

print(f"Stoch Gradient:  {weights_hist[-1,:]}")


fig, axs = plt.subplots(len(weights),1)
for sub in range(len(weights)):
    axs[sub].plot(weights_hist[:,sub])
    axs[sub].axhline(y=true_weights[sub], color="red")
    axs[sub].set_ylim(min(true_weights[sub], weights_hist[:,sub].min()) - 5,
        max(true_weights[sub],weights_hist[:,sub].max()) + 5)
    axs[sub].set_ylabel(f"Est weight {sub}")
axs[len(weights)-1].set_xlabel("Number of iterations")

fig.suptitle(f"SGD: batch size {b}, learning rate {eta}")
plt.show()
